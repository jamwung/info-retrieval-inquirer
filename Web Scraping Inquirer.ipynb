{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14be6bd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T10:52:31.134869Z",
     "start_time": "2022-11-24T10:52:31.128476Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368a0ac",
   "metadata": {},
   "source": [
    "# Auxilliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ba94808",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T10:49:48.326846Z",
     "start_time": "2022-11-24T10:49:48.276906Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Return a BeautifulSoup from the input `url`.\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    proxies = {'http': 'http://206.189.157.23'}\n",
    "    soup = bs4.BeautifulSoup(\n",
    "        requests.get(url, headers=headers, proxies=proxies).content\n",
    "    )\n",
    "    return soup\n",
    "\n",
    "def get_lifestyle(url, cat):\n",
    "    \"\"\"\n",
    "    Get an article in the lifestyle domain of inquirer.net based on `url` and\n",
    "    `cat`.\n",
    "    \"\"\"\n",
    "    title_list = []\n",
    "    text_list = []\n",
    "    link_list = []\n",
    "    publish_time_list = []\n",
    "    time.sleep(0.01)\n",
    "    for fashion in [soup.find_all('h3', class_='title')\n",
    "                    for soup in get_soup(url)\n",
    "                    .find_all('div', id='primary')][0]:\n",
    "        url_article = fashion.find('a').get('href')\n",
    "        text_time = str(get_soup(url_article).find_all('div', class_=\"meta\"))\n",
    "        try:\n",
    "            publish_time = re.findall('\\d\\d\\:\\d\\d\\sPM\\s\\w+\\s\\d\\d\\,\\s\\d{4}',\n",
    "                                      text_time)[0]\n",
    "            publish_time_list.append(publish_time)\n",
    "        except:\n",
    "            continue\n",
    "        title = fashion.find('a').get_text()\n",
    "        title_list.append(title)\n",
    "        link_list.append(url_article)\n",
    "        text = ' '.join([p.text.strip()\n",
    "                         for p in get_soup(url_article)\n",
    "                         .find_all('p')])\n",
    "        text_list.append(text)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['title'] = title_list\n",
    "    df['link'] = link_list\n",
    "    df['publish_time'] = publish_time_list\n",
    "    df['text'] = text_list\n",
    "    df['category'] = cat\n",
    "    return df\n",
    "\n",
    "def get_all_lifestyle(url, df, cat):\n",
    "    \"\"\"\n",
    "    Get the article information based on a `url` and concatenate to `df` in\n",
    "    the given category `cat` from the most recent until the start of 2018.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            time.sleep(0.01)\n",
    "            url_next = (get_soup(url).find('a',\n",
    "                                           class_='next page-numbers')\n",
    "                        .get('href'))\n",
    "            df_next = get_lifestyle(url_next, cat)\n",
    "            df_nextnext = (pd.concat([df, df_next],\n",
    "                                     ignore_index=True)\n",
    "                           .drop_duplicates())\n",
    "            if int(df_nextnext['publish_time'].values[-1][-4:]) >= 2018:\n",
    "                df = df_nextnext.copy()\n",
    "                url = url_next \n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    return df_nextnext\n",
    "\n",
    "def get_lifestyle_df(lifestyle):\n",
    "    \"\"\"\n",
    "    Label the lifestyle articles by category and return the corresponding\n",
    "    DataFrame.\n",
    "    \"\"\"\n",
    "    df_lifestyle = pd.DataFrame()\n",
    "    time.sleep(0.01)\n",
    "    for life in list(lifestyle.keys()):\n",
    "        url = lifestyle[life]\n",
    "        cat = life\n",
    "        df = get_lifestyle(url, cat)\n",
    "        df_life = get_all_lifestyle(url, df, cat)\n",
    "        df_lifestyle = pd.concat([df_lifestyle, df_life], ignore_index=True)\n",
    "    return df_lifestyle\n",
    "\n",
    "def get_all_lifestyle_search(url):\n",
    "    \"\"\"\n",
    "    Collect all titles of articles from `url`.\n",
    "    \"\"\"\n",
    "    title_list = []\n",
    "    time.sleep(0.01)\n",
    "    while True:\n",
    "        try:\n",
    "            soup = (get_soup(url).find_all('div',\n",
    "                               class_=\"block-wrap block-wrap-24 block-css-0\"\n",
    "                               \" block-wrap-classic block-wrap-no-9 elements\"\n",
    "                               \"-design-1 block-skin-0 tipi-box block-mason\"\n",
    "                               \"ry-style block-masonry-no-v block-masonry-\"\n",
    "                               \"design-1 block-masonry-wrap clearfix\")[0])\n",
    "            more_title_list = [sabaw.get_text()\n",
    "                               for sabaw in soup\n",
    "                               .find_all('h3', class_=\"title\")]\n",
    "            title_list.extend(more_title_list)\n",
    "            url = (get_soup(url).find('a',\n",
    "                                     class_='inf-load-more block-loader'\n",
    "                                     ' tipi-button tipi-button-border')\n",
    "            .get('href'))\n",
    "        except:\n",
    "            break\n",
    "    return title_list\n",
    "\n",
    "def get_lifestyle_search(soup):\n",
    "    \"\"\"\n",
    "    Extract the relevant information on an article based on `soup`.\n",
    "    \"\"\"\n",
    "    title_list = []\n",
    "    text_list = []\n",
    "    link_list = []\n",
    "    publish_time_list = []\n",
    "    time.sleep(0.01)\n",
    "    for fashion in soup.find_all('h3', class_='title'):\n",
    "        url_article = fashion.find('a').get('href')\n",
    "        text_time = str(get_soup(url_article).find_all('div', class_=\"meta\"))\n",
    "        try:\n",
    "            publish_time = re.findall('\\d\\d\\:\\d\\d\\sPM\\s\\w+\\s\\d\\d\\,\\s\\d{4}',\n",
    "                                      text_time)[0]\n",
    "            publish_time_list.append(publish_time)\n",
    "        except:\n",
    "            continue\n",
    "        title = fashion.find('a').get_text()\n",
    "        title_list.append(title)\n",
    "        link_list.append(url_article)\n",
    "        text = ' '.join([p.text.strip() for p in get_soup(url_article)\n",
    "                         .find_all('p')])\n",
    "        text_list.append(text)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['title'] = title_list\n",
    "    df['link'] = link_list\n",
    "    df['publish_time'] = publish_time_list\n",
    "    df['text'] = text_list\n",
    "    return df\n",
    "\n",
    "def get_mentalhealth_search(url):\n",
    "    \"\"\"\n",
    "    Get the relevant information of mental health related articles and\n",
    "    return the corresponding DataFrame.\n",
    "    \"\"\"\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "    df = pd.DataFrame()\n",
    "    time.sleep(0.01)\n",
    "    while True:\n",
    "        try:\n",
    "            soup = (get_soup(url).find_all('div',\n",
    "                               class_=\"block-wrap block-wrap-24 block-css-0\"\n",
    "                               \" block-wrap-classic block-wrap-no-9 elements\"\n",
    "                               \"-design-1 block-skin-0 tipi-box block-mason\"\n",
    "                               \"ry-style block-masonry-no-v block-masonry-\"\n",
    "                               \"design-1 block-masonry-wrap clearfix\")[0])\n",
    "            df_mental = get_lifestyle_search(soup)\n",
    "            df = pd.concat([df, df_mental], ignore_index=True)\n",
    "            if int(df['publish_time'].values[-1][-4:]) >= 2018:\n",
    "                df = df.copy()\n",
    "            else:\n",
    "                break\n",
    "            url = (get_soup(url).find('a',\n",
    "                                     class_='inf-load-more block-loader'\n",
    "                                     ' tipi-button tipi-button-border')\n",
    "                   .get('href'))\n",
    "        except:\n",
    "            break\n",
    "    return df\n",
    "\n",
    "def create_database():\n",
    "    \"\"\"\n",
    "    Create a SQL Database of the data in inquirer.csv as a table.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(\"inquirer.db\")\n",
    "    c = conn.cursor()\n",
    "    query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS\n",
    "            inquirer (publish_time, title, link, text, mh_related)\n",
    "    \"\"\"\n",
    "    c.execute(query)\n",
    "    inq = pd.read_csv('inquirer.csv')\n",
    "    inq.to_sql('inquirer.db', conn, if_exists='replace', index=False)\n",
    "    conn.commit()\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe81c87",
   "metadata": {},
   "source": [
    "# Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a82f853f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T10:29:47.988922Z",
     "start_time": "2022-11-24T10:29:47.681216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get category keys.\n",
    "with open('inquirer_domains.json') as file:\n",
    "    inq_dict = json.load(file)\n",
    "\n",
    "lifestyle = inq_dict['lifestyle']\n",
    "\n",
    "# Generate a dataframe of lifestyle articles in inquirer.\n",
    "try:\n",
    "    df_lifestyle = pd.read_csv('lifestyle.csv')\n",
    "except:\n",
    "    df_lifestyle = get_lifestyle_df(lifestyle)\n",
    "    df_lifestyle['year'] = (df_lifestyle\n",
    "                            .drop_duplicates()\n",
    "                            ['publish_time']\n",
    "                            .apply(lambda x: int(x[-4:])))\n",
    "    df_lifestyle.loc[df_lifestyle['year'] >= 2018].to_csv('lifestyle.csv',\n",
    "                                                          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dc2214c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T10:29:42.894216Z",
     "start_time": "2022-11-24T10:29:42.821865Z"
    }
   },
   "outputs": [],
   "source": [
    "url_search = 'https://lifestyle.inquirer.net/?s=mental+health'\n",
    "\n",
    "# Get list of articles related to mental health.\n",
    "try:\n",
    "    df_mentalheath = pd.read_csv('mental_health_articles.csv')\n",
    "except:\n",
    "    df_mentalhealth = get_mentalhealth_search(url_search)\n",
    "    df_mentalheath.to_csv('mental_health_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156f1245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T10:29:49.412921Z",
     "start_time": "2022-11-24T10:29:49.399345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check mental health related articles that are in the corpus.\n",
    "df_lifestyle.loc[df_lifestyle['title']\n",
    "                 .isin(df_mentalheath['title']\n",
    "                       .tolist()), 'mh_related'] = 1\n",
    "\n",
    "df_lifestyle['mh_related'] = df_lifestyle['mh_related'].replace(np.nan, 0)\n",
    "df_mentalheath['mh_related'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4b36d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T10:29:51.324572Z",
     "start_time": "2022-11-24T10:29:50.843299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate the dataframes and construct the complete corpus.\n",
    "inq = pd.concat([df_lifestyle, df_mentalheath]).drop(columns=['category',\n",
    "                                                              'year'])\n",
    "inq.mh_related = inq.mh_related.astype(int)\n",
    "inq = inq.drop_duplicates()\n",
    "\n",
    "inq.publish_time = pd.to_datetime(inq.publish_time,\n",
    "                                  format='%I:%M %p %B %d, %Y')\n",
    "\n",
    "inq = (inq[['publish_time', 'title', 'link', 'text', 'mh_related']]\n",
    "       .sort_values('publish_time', ascending=False).reset_index(drop=True))\n",
    "\n",
    "with open('inquirer.csv', 'w') as f:\n",
    "    inq.to_csv(f, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
